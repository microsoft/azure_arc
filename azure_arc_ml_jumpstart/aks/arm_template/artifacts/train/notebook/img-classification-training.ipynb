{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial #1: Train an image classification model with Azure Arc-enabled Machine Learning\n",
        "\n",
        "In this tutorial, we train a machine learning model on Azure Arc-enabled Kubernetes cluster. We'll use the training and deployment workflow for Azure Machine Learning service in a Python Jupyter notebook.  This notebook can be used as a template to train any model with custom data.  \n",
        "\n",
        "This tutorial trains a simple logistic regression using the [MNIST](https://azure.microsoft.com/services/open-datasets/catalog/mnist/) dataset and [scikit-learn](http://scikit-learn.org) with Azure Machine Learning.  MNIST is a popular dataset consisting of 70,000 grayscale images. Each image is a handwritten digit of 28x28 pixels, representing a number from 0 to 9. The goal is to create a multi-class classifier to identify the digit a given image represents. \n",
        "\n",
        "Here is what we are demonstrating:\n",
        "![Architecture diagram](https://rakirahman.blob.core.windows.net/public/images/Misc/AML-Arc-amlUI.png)\n",
        "\n",
        "#### 1. Leverage Azure ML existing SDK to train an ML model\n",
        "#### 2. Submit the training job to a remote Kubernetes Cluster (e.g. Arc-connected On-Premises OpenShift Cluster)\n",
        "#### 3. Arc Cluster communicates outbound to Azure Control Plane\n",
        "#### 4. Training Container Image created with Conda dependencies, injected into Customer's Private Azure Container Registry (ACR)\n",
        "#### 5. The Training Pod spins up with all the necessary pre-reqs, using the curated container image that just got created in ACR\n",
        "\n",
        "> The idea here is, a Data Scientist that needs access to a ready-to-go DEV machine (that has Python, Azure ML SDK etc installed) so they can do their work - **without** loading sensitive data into this DEV machine. I.e. they'll use this machine to interact with Azure ML, whilst the actual training with Sensitive Data happens on Kubernetes.\n",
        "\n",
        "> <br> <br> The Data Scientist could just as well use VS Code on their laptop to perform everything in this Notebook, provided they have the local tooling installed."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Import packages\n",
        "\n",
        "Import Python packages you need in this session. Also display the Azure Machine Learning SDK version."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import azureml.core\n",
        "from azureml.core import Workspace\n",
        "\n",
        "# check core SDK version number, note AML Python SDK 1.30 or above is required for this sample notebook\n",
        "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "tags": [
          "check version"
        ],
        "gather": {
          "logged": 1630958447372
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Connect to workspace\n",
        "\n",
        "Create a workspace object from the existing workspace. `Workspace.from_config()` reads the file **config.json** and loads the details into an object named `ws`."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Load workspace configuration\n",
        "ws = Workspace.from_config()\n",
        "print(ws.name, ws.location, ws.resource_group, ws.subscription_id, sep='\\t')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "tags": [
          "load workspace"
        ],
        "gather": {
          "logged": 1630958449887
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create experiment\n",
        "\n",
        "Create an experiment to track the runs in your workspace. A workspace can have muliple experiments. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "experiment_name = 'Tutorial-sklearn-mnist-amlui'\n",
        "\n",
        "from azureml.core import Experiment\n",
        "exp = Experiment(workspace=ws, name=experiment_name)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "tags": [
          "create experiment"
        ],
        "gather": {
          "logged": 1630958451308
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see:\r\n",
        "![Experiment](https://i.imgur.com/zaQH78T.png)"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create compute target - attach Azure Arc-enabled Kubernetes cluster to AML workspace\n",
        "\n",
        "> Note that you would need Azure Arc-enabled Kuberenetes cluster resource ID to create a Kubernetes compute target. Talk to your workspace or IT admin if you don't have an Azure Arc-enabled Kubernetes cluster, see [Azure Arc-enable Machine Learning instruction](link to documentation).\n",
        "\n",
        "By using Azure Arc-enabled KubernetesCompute, a customer-managed service, data scientists can train machine learning models on Kubernetes cluster anywhere, in cloud or on-premise (no Kubernetes expertise needed). \n",
        "\n",
        "The code below attaches the Azure Arc-enabled Kubernetes cluster for you if they don't already exist in your workspace.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.compute import KubernetesCompute\n",
        "from azureml.core.compute import ComputeTarget\n",
        "import os\n",
        "\n",
        "# choose a name for your Azure Arc-enabled Kubernetes compute\n",
        "amlarc_compute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"Arc-AML-AKS\")\n",
        "\n",
        "if amlarc_compute_name in ws.compute_targets:\n",
        "    amlarc_compute = ws.compute_targets[amlarc_compute_name]\n",
        "    if amlarc_compute and type(amlarc_compute) is KubernetesCompute:\n",
        "        print(\"found compute target: \" + amlarc_compute_name)\n",
        "else:\n",
        "    print(\"do this via az cli...\")\n",
        "    # ... "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "tags": [
          "create mlc",
          "amlcompute"
        ],
        "gather": {
          "logged": 1630958455066
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have the necessary packages and compute resources to train a model in Arc-enabled Kubernetes. \n",
        "\n",
        "# 1. Leverage Azure ML existing SDK to train an ML model\n",
        "\n",
        "## Explore data\n",
        "\n",
        "Before you train a model, you need to understand the data that you are using to train it. In this section you learn how to:\n",
        "\n",
        "* Download the MNIST dataset\n",
        "* Display some sample images\n",
        "\n",
        "### Download the MNIST dataset\n",
        "\n",
        "Use Azure Open Datasets to get the raw MNIST data files. [Azure Open Datasets](https://docs.microsoft.com/azure/open-datasets/overview-what-are-open-datasets) are curated public datasets that you can use to add scenario-specific features to machine learning solutions for more accurate models. Each dataset has a corrseponding class, `MNIST` in this case, to retrieve the data in different ways.\n",
        "\n",
        "This code retrieves the data as a `FileDataset` object, which is a subclass of `Dataset`. A `FileDataset` references single or multiple files of any format in your datastores or public urls. The class provides you with the ability to download or mount the files to your compute by creating a reference to the data source location. Additionally, you register the Dataset to your workspace for easy retrieval during training.\n",
        "\n",
        "Follow the [how-to](https://aka.ms/azureml/howto/createdatasets) to learn more about Datasets and their usage in the SDK."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Dataset\n",
        "from azureml.opendatasets import MNIST\n",
        "\n",
        "data_folder = os.path.join(os.getcwd(), 'data')\n",
        "os.makedirs(data_folder, exist_ok=True)\n",
        "\n",
        "mnist_file_dataset = MNIST.get_file_dataset()\n",
        "mnist_file_dataset.download(data_folder, overwrite=True)\n",
        "\n",
        "mnist_file_dataset = mnist_file_dataset.register(workspace=ws,\n",
        "                                                 name='mnist_opendataset',\n",
        "                                                 description='training and test dataset',\n",
        "                                                 create_new_version=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1630958472274
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ls data/https%3A/%2Fazureopendatastorage.azurefd.net/mnist/"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630958472545
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630958485897
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Display some sample images\n",
        "\n",
        "Load the compressed files into `numpy` arrays. Then use `matplotlib` to plot 30 random images from the dataset with their labels above them. Note this step requires a `load_data` function that's included in an `utils.py` file. This file is included in the sample folder. Please make sure it is placed in the same folder as this notebook. The `load_data` function simply parses the compresse files into numpy arrays."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# make sure utils.py is in the same directory as this code\n",
        "from utils import load_data\n",
        "import glob\n",
        "\n",
        "\n",
        "# note we also shrink the intensity values (X) from 0-255 to 0-1. This helps the model converge faster.\n",
        "X_train = load_data(glob.glob(os.path.join(data_folder,\"**/train-images-idx3-ubyte.gz\"), recursive=True)[0], False) / 255.0\n",
        "X_test = load_data(glob.glob(os.path.join(data_folder,\"**/t10k-images-idx3-ubyte.gz\"), recursive=True)[0], False) / 255.0\n",
        "y_train = load_data(glob.glob(os.path.join(data_folder,\"**/train-labels-idx1-ubyte.gz\"), recursive=True)[0], True).reshape(-1)\n",
        "y_test = load_data(glob.glob(os.path.join(data_folder,\"**/t10k-labels-idx1-ubyte.gz\"), recursive=True)[0], True).reshape(-1)\n",
        "\n",
        "\n",
        "# now let's show some randomly chosen images from the traininng set.\n",
        "count = 0\n",
        "sample_size = 30\n",
        "plt.figure(figsize = (16, 6))\n",
        "for i in np.random.permutation(X_train.shape[0])[:sample_size]:\n",
        "    count = count + 1\n",
        "    plt.subplot(1, sample_size, count)\n",
        "    plt.axhline('')\n",
        "    plt.axvline('')\n",
        "    plt.text(x=10, y=-10, s=y_train[i], fontsize=18)\n",
        "    plt.imshow(X_train[i].reshape(28, 28), cmap=plt.cm.Greys)\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1630958492717
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Submit the training job to a remote Kubernetes Cluster (e.g. Arc-connected On-Premises OpenShift Cluster)\n",
        "\n",
        "For this task, you submit the job to run on the Azure Arc-enabled Kuberenetes cluster you set up earlier.  To submit a job you:\n",
        "* Create a directory\n",
        "* Create a training script\n",
        "* Create a script run configuration\n",
        "* Submit the job \n",
        "\n",
        "### Create a directory\n",
        "\n",
        "Create a directory to deliver the necessary code from your computer to the remote resource."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "script_folder = os.path.join(os.getcwd(), \"sklearn-mnist\")\n",
        "os.makedirs(script_folder, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1630958517102
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a training script\n",
        "\n",
        "To submit the job to the cluster, first create a training script. Run the following code to create the training script called `train.py` in the directory you just created. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $script_folder/train.py\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "import glob\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import joblib\n",
        "\n",
        "from azureml.core import Run\n",
        "from utils import load_data\n",
        "\n",
        "# let user feed in 2 parameters, the dataset to mount or download, and the regularization rate of the logistic regression model\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--data-folder', type=str, dest='data_folder', help='data folder mounting point')\n",
        "parser.add_argument('--regularization', type=float, dest='reg', default=0.01, help='regularization rate')\n",
        "args = parser.parse_args()\n",
        "\n",
        "data_folder = args.data_folder\n",
        "print('Data folder:', data_folder)\n",
        "\n",
        "# load train and test set into numpy arrays\n",
        "# note we scale the pixel intensity values to 0-1 (by dividing it with 255.0) so the model can converge faster.\n",
        "X_train = load_data(glob.glob(os.path.join(data_folder, '**/train-images-idx3-ubyte.gz'), recursive=True)[0], False) / 255.0\n",
        "X_test = load_data(glob.glob(os.path.join(data_folder, '**/t10k-images-idx3-ubyte.gz'), recursive=True)[0], False) / 255.0\n",
        "y_train = load_data(glob.glob(os.path.join(data_folder, '**/train-labels-idx1-ubyte.gz'), recursive=True)[0], True).reshape(-1)\n",
        "y_test = load_data(glob.glob(os.path.join(data_folder, '**/t10k-labels-idx1-ubyte.gz'), recursive=True)[0], True).reshape(-1)\n",
        "\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape, sep = '\\n')\n",
        "\n",
        "# get hold of the current run\n",
        "run = Run.get_context()\n",
        "\n",
        "print('Train a logistic regression model with regularization rate of', args.reg)\n",
        "clf = LogisticRegression(C=1.0/args.reg, solver=\"liblinear\", multi_class=\"auto\", random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "print('Predict the test set')\n",
        "y_hat = clf.predict(X_test)\n",
        "\n",
        "# calculate accuracy on the prediction\n",
        "acc = np.average(y_hat == y_test)\n",
        "print('Accuracy is', acc)\n",
        "\n",
        "run.log('regularization rate', np.float(args.reg))\n",
        "run.log('accuracy', np.float(acc))\n",
        "\n",
        "os.makedirs('outputs', exist_ok=True)\n",
        "# note file saved in the outputs folder is automatically uploaded into experiment record\n",
        "joblib.dump(value=clf, filename='outputs/sklearn_mnist_model.pkl')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The file `utils.py` is referenced from the training script to load the dataset correctly.  Copy this script into the script folder so that it can be accessed along with the training script on the remote resource."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.copy('utils.py', script_folder)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1630958542075
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure the training job\n",
        "\n",
        "Create a ScriptRunConfig object to specify the configuration details of your training job, including your training script, environment to use, and the compute target to run on. Configure the ScriptRunConfig by specifying:\n",
        "\n",
        "* The directory that contains your scripts. All the files in this directory are uploaded into the cluster nodes for execution. \n",
        "* The compute target.  In this case you will use the AmlCompute you created\n",
        "* The training script name, train.py\n",
        "* An environment that contains the libraries needed to run the script\n",
        "* Arguments required from the training script. \n",
        "\n",
        "In this tutorial, the target is `KubernetesCompute`. All files in the script folder are uploaded into the cluster nodes for execution. The data_folder is set to use the dataset.\n",
        "\n",
        "First, create the environment that contains: the scikit-learn library, azureml-dataset-runtime required for accessing the dataset, and azureml-defaults which contains the dependencies for logging metrics.\n",
        "\n",
        "Once the environment is defined, register it with the Workspace to re-use later."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.environment import Environment\n",
        "from azureml.core.conda_dependencies import CondaDependencies\n",
        "\n",
        "# to install required packages\n",
        "env = Environment('tutorial-env')\n",
        "cd = CondaDependencies.create(pip_packages=['azureml-dataset-runtime[pandas,fuse]', 'azureml-defaults'], conda_packages = ['scikit-learn==0.22.1'])\n",
        "\n",
        "env.python.conda_dependencies = cd\n",
        "\n",
        "# Register environment to re-use later\n",
        "env.register(workspace = ws)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1630958596558
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, create the ScriptRunConfig by specifying the training script, compute target and environment."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import ScriptRunConfig\n",
        "\n",
        "args = ['--data-folder', mnist_file_dataset.as_mount(), '--regularization', 0.5]\n",
        "\n",
        "src = ScriptRunConfig(source_directory=script_folder,\n",
        "                      script='train.py', \n",
        "                      arguments=args,\n",
        "                      compute_target=amlarc_compute,\n",
        "                      environment=env)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "tags": [
          "configure estimator"
        ],
        "gather": {
          "logged": 1630958608690
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Submit the job to the Azure Arc-enabled Kubernetes cluster\n",
        "\n",
        "Run the experiment by submitting the ScriptRunConfig object. And you can navigate to Azure portal to monitor the run."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "run = exp.submit(config=src)\n",
        "run"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "tags": [
          "remote run",
          "amlcompute",
          "scikit-learn"
        ],
        "gather": {
          "logged": 1630958624784
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see:\r\n",
        "\r\n",
        "# 3. Arc Cluster communicates outbound to Azure Control Plane\r\n",
        "![Run 1](https://i.imgur.com/vhI67L5.png)\r\n",
        "\r\n",
        "# 4. Training Container Image created with Conda dependencies, injected into Customer's Private Azure Container Registry (ACR)\r\n",
        "Image build logs:\r\n",
        "![Image build](https://i.imgur.com/rhG3br2.png)\r\n",
        " \r\n",
        "Container build:\r\n",
        "![Container build](https://i.imgur.com/OpevDVP.png)\r\n",
        " \r\n",
        "# 5. The Training Pod spins up with all the necessary pre-reqs, using the curated container image that just got created in ACR\r\n",
        "Goes into queued:\r\n",
        "![Job stage](https://i.imgur.com/6IK2gmE.png)"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the call is asynchronous, it returns a **Preparing** or **Running** state as soon as the job is started.\n",
        "\n",
        "## Monitor a remote run\n",
        "\n",
        "In total, the first run takes **approximately 10 minutes**. But for subsequent runs, as long as the dependencies in the Azure ML environment don't change, the same image is reused and hence the container start up time is much faster.\n",
        "\n",
        "Here is what's happening while we wait:\n",
        "\n",
        "- **Image creation**: A Docker image is created matching the Python environment specified by the Azure ML environment. The image is built and stored in the ACR (Azure Container Registry) associated with your workspace. Image creation and uploading takes **about 5 minutes**. \n",
        "\n",
        "  This stage happens once for each Python environment since the container is cached for subsequent runs.  During image creation, logs are streamed to the run history. You can monitor the image creation progress using these logs.\n",
        "\n",
        "- **Running**: In this stage, the necessary scripts and files are sent to the compute target, then data stores are mounted/copied, then the entry_script is run. While the job is running, stdout and the files in the ./logs directory are streamed to the run history. You can monitor the run's progress using these logs.\n",
        "\n",
        "- **Post-Processing**: The ./outputs directory of the run is copied over to the run history in your workspace so you can access these results.\n",
        "\n",
        "\n",
        "You can check the progress of a running job in multiple ways. This tutorial uses a Jupyter widget as well as a `wait_for_completion` method. \n",
        "\n",
        "### Jupyter widget\n",
        "\n",
        "Watch the progress of the run with a Jupyter widget.  Like the run submission, the widget is asynchronous and provides live updates every 10-15 seconds until the job completes."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.widgets import RunDetails\n",
        "RunDetails(run).show()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Class KubernetesCompute: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
        }
      ],
      "execution_count": null,
      "metadata": {
        "tags": [
          "use notebook widget"
        ],
        "gather": {
          "logged": 1630958656554
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get log results upon completion\n",
        "\n",
        "Model training happens in the background. You can use `wait_for_completion` to block and wait until the model has completed training before running more code. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# specify show_output to True for a verbose log\n",
        "run.wait_for_completion(show_output=True) "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "tags": [
          "remote run",
          "amlcompute",
          "scikit-learn"
        ],
        "gather": {
          "logged": 1630959431952
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Display run results\n",
        "\n",
        "You now have a model trained on a remote cluster.  Retrieve all the metrics logged during the run, including the accuracy of the model:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(run.get_metrics())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "tags": [
          "get metrics"
        ],
        "gather": {
          "logged": 1630959432531
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From here, we can deploy our trained model as a live inference endpoint, retrain etc."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "authors": [
      {
        "name": "maxluk"
      }
    ],
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "network_required": false,
    "kernel_info": {
      "name": "python3-azureml"
    },
    "msauthor": "roastala",
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "categories": [
      "tutorials",
      "image-classification-mnist-data"
    ],
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}